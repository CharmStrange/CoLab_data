{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOMr1Fw5tfj8OUuTK6YmXwB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CharmStrange/CoLab_data/blob/main/ipynbs/D.E./PySpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FebmXvFYl74w",
        "outputId": "5d776078-1b27-4cd3-a7cd-8bb88a4f6dd3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=99bec8b63000b0f3c99ffa98bb307f5ccac79bac6fc92143689feb641b3e0678\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime, date\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "df = spark.createDataFrame([\n",
        "    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n",
        "    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n",
        "    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n",
        "])\n",
        "\"\"\"\n",
        "동일한 방\n",
        "df = spark.createDataFrame([\n",
        "    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n",
        "    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n",
        "    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n",
        "], schema='a long, b double, c string, d date, e timestamp')\n",
        "df\n",
        "\"\"\"\n",
        "\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoBlBkSal66w",
        "outputId": "a3f4f2b2-4c14-42c3-eb96-afb951bfc3ff"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pandas_df = pd.DataFrame({\n",
        "    'a': [1, 2, 3],\n",
        "    'b': [2., 3., 4.],\n",
        "    'c': ['string1', 'string2', 'string3'],\n",
        "    'd': [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)],\n",
        "    'e': [datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)]\n",
        "})\n",
        "df = spark.createDataFrame(pandas_df)\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJQZN8RLmdVX",
        "outputId": "42e8d068-18c9-4fb4-c1ef-56ad5ec7a687"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# All DataFrames above result same.\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kULyjxQqmpkv",
        "outputId": "de722423-fe99-474f-fb9e-d5aa4f1bc941"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+-------+----------+-------------------+\n",
            "|  a|  b|      c|         d|                  e|\n",
            "+---+---+-------+----------+-------------------+\n",
            "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
            "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
            "|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
            "+---+---+-------+----------+-------------------+\n",
            "\n",
            "root\n",
            " |-- a: long (nullable = true)\n",
            " |-- b: double (nullable = true)\n",
            " |-- c: string (nullable = true)\n",
            " |-- d: date (nullable = true)\n",
            " |-- e: timestamp (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjtW57YZmwSv",
        "outputId": "1d5b7873-1425-4a76-81ee-5d4f0b18d4c1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.dataframe.DataFrame"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Testing your PySpark Application"
      ],
      "metadata": {
        "id": "SHGb_2c1n4d_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pkg/etl.py\n",
        "import unittest\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.functions import regexp_replace\n",
        "from pyspark.testing.utils import assertDataFrameEqual\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"Sample PySpark ETL\").getOrCreate()\n",
        "\n",
        "sample_data = [{\"name\": \"John    D.\", \"age\": 30},\n",
        "  {\"name\": \"Alice   G.\", \"age\": 25},\n",
        "  {\"name\": \"Bob  T.\", \"age\": 35},\n",
        "  {\"name\": \"Eve   A.\", \"age\": 28}]\n",
        "\n",
        "df = spark.createDataFrame(sample_data)\n",
        "\n",
        "# Define DataFrame transformation function\n",
        "def remove_extra_spaces(df, column_name):\n",
        "    # Remove extra spaces from the specified column using regexp_replace\n",
        "    df_transformed = df.withColumn(column_name, regexp_replace(col(column_name), \"\\\\s+\", \" \"))\n",
        "\n",
        "    return df_transformed\n",
        "\n",
        "# pkg/test_etl.py\n",
        "import unittest\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Define unit test base class\n",
        "class PySparkTestCase(unittest.TestCase):\n",
        "    @classmethod\n",
        "    def setUpClass(cls):\n",
        "        cls.spark = SparkSession.builder.appName(\"Sample PySpark ETL\").getOrCreate()\n",
        "\n",
        "    @classmethod\n",
        "    def tearDownClass(cls):\n",
        "        cls.spark.stop()\n",
        "\n",
        "# Define unit test\n",
        "class TestTranformation(PySparkTestCase):\n",
        "    def test_single_space(self):\n",
        "        sample_data = [{\"name\": \"John    D.\", \"age\": 30},\n",
        "                        {\"name\": \"Alice   G.\", \"age\": 25},\n",
        "                        {\"name\": \"Bob  T.\", \"age\": 35},\n",
        "                        {\"name\": \"Eve   A.\", \"age\": 28}]\n",
        "\n",
        "        # Create a Spark DataFrame\n",
        "        original_df = spark.createDataFrame(sample_data)\n",
        "\n",
        "        # Apply the transformation function from before\n",
        "        transformed_df = remove_extra_spaces(original_df, \"name\")\n",
        "\n",
        "        expected_data = [{\"name\": \"John D.\", \"age\": 30},\n",
        "        {\"name\": \"Alice G.\", \"age\": 25},\n",
        "        {\"name\": \"Bob T.\", \"age\": 35},\n",
        "        {\"name\": \"Eve A.\", \"age\": 28}]\n",
        "\n",
        "        expected_df = spark.createDataFrame(expected_data)\n",
        "\n",
        "        assertDataFrameEqual(transformed_df, expected_df)\n"
      ],
      "metadata": {
        "id": "pEIBd05Vn4I3"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}